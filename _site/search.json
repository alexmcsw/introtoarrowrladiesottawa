[
  {
    "objectID": "slides.html#what-is-apache-arrow",
    "href": "slides.html#what-is-apache-arrow",
    "title": "Intro to Arrow in R",
    "section": "What is Apache Arrow?",
    "text": "What is Apache Arrow?\n\n\n\nA multi-language toolbox for accelerated data interchange and in-memory processing\n\n\n\nArrow is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another\n\n\n\n\nhttps://arrow.apache.org/overview/"
  },
  {
    "objectID": "slides.html#apache-arrow-specification",
    "href": "slides.html#apache-arrow-specification",
    "title": "Intro to Arrow in R",
    "section": "Apache Arrow Specification",
    "text": "Apache Arrow Specification\nIn-memory columnar format: a standardized, language-agnostic specification for representing structured, table-like data sets in-memory."
  },
  {
    "objectID": "slides.html#a-multi-language-toolbox",
    "href": "slides.html#a-multi-language-toolbox",
    "title": "Intro to Arrow in R",
    "section": "A Multi-Language Toolbox",
    "text": "A Multi-Language Toolbox"
  },
  {
    "objectID": "slides.html#accelerated-data-interchange",
    "href": "slides.html#accelerated-data-interchange",
    "title": "Intro to Arrow in R",
    "section": "Accelerated Data Interchange",
    "text": "Accelerated Data Interchange"
  },
  {
    "objectID": "slides.html#accelerated-in-memory-processing",
    "href": "slides.html#accelerated-in-memory-processing",
    "title": "Intro to Arrow in R",
    "section": "Accelerated In-Memory Processing",
    "text": "Accelerated In-Memory Processing\nArrow‚Äôs Columnar Format is Fast\n\n\nThe contiguous columnar layout enables vectorization using the latest SIMD (Single Instruction, Multiple Data) operations included in modern processors."
  },
  {
    "objectID": "slides.html#arrow",
    "href": "slides.html#arrow",
    "title": "Intro to Arrow in R",
    "section": "arrow üì¶",
    "text": "arrow üì¶"
  },
  {
    "objectID": "slides.html#arrow-1",
    "href": "slides.html#arrow-1",
    "title": "Intro to Arrow in R",
    "section": "arrow üì¶",
    "text": "arrow üì¶"
  },
  {
    "objectID": "slides.html#seattle-checkouts-big-csv",
    "href": "slides.html#seattle-checkouts-big-csv",
    "title": "Intro to Arrow in R",
    "section": "SeattleCheckoutsBig CSV",
    "text": "SeattleCheckoutsBig CSV"
  },
  {
    "objectID": "slides.html#how-big-is-the-dataset",
    "href": "slides.html#how-big-is-the-dataset",
    "title": "Intro to Arrow in R",
    "section": "How big is the dataset?",
    "text": "How big is the dataset?\n\nfs::file_size(\"./data/seattle-library-checkouts.csv\")\n\n8.58G"
  },
  {
    "objectID": "slides.html#opening-in-arrow",
    "href": "slides.html#opening-in-arrow",
    "title": "Intro to Arrow in R",
    "section": "Opening in Arrow",
    "text": "Opening in Arrow\n\nseattle_csv &lt;- open_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n)"
  },
  {
    "objectID": "slides.html#extract-schema",
    "href": "slides.html#extract-schema",
    "title": "Intro to Arrow in R",
    "section": "Extract schema",
    "text": "Extract schema\n\nschema(seattle_csv)\n\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string"
  },
  {
    "objectID": "slides.html#arrow-data-types",
    "href": "slides.html#arrow-data-types",
    "title": "Intro to Arrow in R",
    "section": "Arrow Data Types",
    "text": "Arrow Data Types\nArrow has a rich data type system, including direct analogs of many R data types\n\n&lt;dbl&gt; == &lt;double&gt;\n&lt;chr&gt; == &lt;string&gt; or &lt;utf8&gt;\n&lt;int&gt; == &lt;int32&gt;\n\n\nhttps://arrow.apache.org/docs/r/articles/data_types.html"
  },
  {
    "objectID": "slides.html#parsing-the-metadata",
    "href": "slides.html#parsing-the-metadata",
    "title": "Intro to Arrow in R",
    "section": "Parsing the Metadata",
    "text": "Parsing the Metadata\n\nArrow scans üëÄ a few thousand rows of the file(s) to impute or ‚Äúguess‚Äù the data types\n\nüìö arrow vs readr blog post: https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/"
  },
  {
    "objectID": "slides.html#parsers-are-not-always-right",
    "href": "slides.html#parsers-are-not-always-right",
    "title": "Intro to Arrow in R",
    "section": "Parsers Are Not Always Right",
    "text": "Parsers Are Not Always Right\n\nschema(seattle_csv)\n\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n\n\n\n\nInternational Standard Book Number (ISBN) is a 13-digit number that uniquely identifies books and book-like products published internationally.\nData Dictionaries, metadata in data catalogues should provide this info."
  },
  {
    "objectID": "slides.html#lets-control-the-schema",
    "href": "slides.html#lets-control-the-schema",
    "title": "Intro to Arrow in R",
    "section": "Let‚Äôs Control the Schema",
    "text": "Let‚Äôs Control the Schema\n\nCreating a schema manually:\n\nschema(\n  UsageClass = utf8(),\n  CheckoutType = utf8(),\n  MaterialType = utf8(),\n  ...\n)\n\n\nThis will take a lot of typing with 12 columns üò¢"
  },
  {
    "objectID": "slides.html#lets-control-the-schema-1",
    "href": "slides.html#lets-control-the-schema-1",
    "title": "Intro to Arrow in R",
    "section": "Let‚Äôs Control the Schema",
    "text": "Let‚Äôs Control the Schema\n\nseattle_csv &lt;- open_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)"
  },
  {
    "objectID": "slides.html#previewing-the-data",
    "href": "slides.html#previewing-the-data",
    "title": "Intro to Arrow in R",
    "section": "Previewing the data",
    "text": "Previewing the data\n\nseattle_csv |&gt; glimpse()\n\nFileSystemDataset with 1 csv file\n41,389,465 rows x 12 columns\n$ UsageClass      &lt;string&gt; \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Physi‚Ä¶\n$ CheckoutType    &lt;string&gt; \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Horizo‚Ä¶\n$ MaterialType    &lt;string&gt; \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOOK\",‚Ä¶\n$ CheckoutYear     &lt;int64&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016,‚Ä¶\n$ CheckoutMonth    &lt;int64&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,‚Ä¶\n$ Checkouts        &lt;int64&gt; 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2, 3,‚Ä¶\n$ Title           &lt;string&gt; \"Super rich : a guide to having it all / Russell Simm‚Ä¶\n$ ISBN            &lt;string&gt; \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"‚Ä¶\n$ Creator         &lt;string&gt; \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim Par‚Ä¶\n$ Subjects        &lt;string&gt; \"Self realization, Conduct of life, Attitude Psycholo‚Ä¶\n$ Publisher       &lt;string&gt; \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Dial ‚Ä¶\n$ PublicationYear &lt;string&gt; \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c2005.‚Ä¶"
  },
  {
    "objectID": "slides.html#arrow-dplyr-backend",
    "href": "slides.html#arrow-dplyr-backend",
    "title": "Intro to Arrow in R",
    "section": "Arrow dplyr backend",
    "text": "Arrow dplyr backend"
  },
  {
    "objectID": "slides.html#querying-the-data",
    "href": "slides.html#querying-the-data",
    "title": "Intro to Arrow in R",
    "section": "Querying the data",
    "text": "Querying the data\n\nseattle_csv |&gt;\n  mutate(IsBook = endsWith(MaterialType, \"BOOK\")) |&gt;\n  select(MaterialType, IsBook)\n\nFileSystemDataset (query)\nMaterialType: string\nIsBook: bool (ends_with(MaterialType, {pattern=\"BOOK\", ignore_case=false}))\n\nSee $.data for the source Arrow object"
  },
  {
    "objectID": "slides.html#preview-the-query",
    "href": "slides.html#preview-the-query",
    "title": "Intro to Arrow in R",
    "section": "Preview the query",
    "text": "Preview the query\n\nseattle_csv |&gt;\n  head(20) |&gt;\n  mutate(IsBook = endsWith(MaterialType, \"BOOK\")) |&gt;\n  select(MaterialType, IsBook) |&gt;\n  collect()\n\n# A tibble: 20 √ó 2\n   MaterialType IsBook\n   &lt;chr&gt;        &lt;lgl&gt; \n 1 BOOK         TRUE  \n 2 BOOK         TRUE  \n 3 EBOOK        TRUE  \n 4 BOOK         TRUE  \n 5 SOUNDDISC    FALSE \n 6 BOOK         TRUE  \n 7 BOOK         TRUE  \n 8 EBOOK        TRUE  \n 9 BOOK         TRUE  \n10 EBOOK        TRUE  \n11 BOOK         TRUE  \n12 BOOK         TRUE  \n13 BOOK         TRUE  \n14 AUDIOBOOK    TRUE  \n15 BOOK         TRUE  \n16 EBOOK        TRUE  \n17 SOUNDDISC    FALSE \n18 VIDEODISC    FALSE \n19 SOUNDDISC    FALSE \n20 AUDIOBOOK    TRUE"
  },
  {
    "objectID": "slides.html#data-manipulation",
    "href": "slides.html#data-manipulation",
    "title": "Intro to Arrow in R",
    "section": "Data manipulation",
    "text": "Data manipulation\n\nseattle_csv |&gt;\n  filter(endsWith(MaterialType, \"BOOK\")) |&gt;\n  group_by(CheckoutYear) |&gt;\n  summarise(Checkouts = sum(Checkouts)) |&gt;\n  arrange(CheckoutYear) |&gt; \n  collect()\n\n# A tibble: 18 √ó 2\n   CheckoutYear Checkouts\n          &lt;int&gt;     &lt;int&gt;\n 1         2005   2129128\n 2         2006   3385869\n 3         2007   3679981\n 4         2008   4156859\n 5         2009   4500788\n 6         2010   4389760\n 7         2011   4484366\n 8         2012   4696376\n 9         2013   5394411\n10         2014   5606168\n11         2015   5784864\n12         2016   5915722\n13         2017   6280679\n14         2018   6831226\n15         2019   7339010\n16         2020   5549585\n17         2021   6659627\n18         2022   6301822"
  },
  {
    "objectID": "slides.html#gb-csv-file-arrow-dplyr",
    "href": "slides.html#gb-csv-file-arrow-dplyr",
    "title": "Intro to Arrow in R",
    "section": "9GB CSV file + arrow + dplyr",
    "text": "9GB CSV file + arrow + dplyr\n\nseattle_csv |&gt;\n  filter(endsWith(MaterialType, \"BOOK\")) |&gt;\n  group_by(CheckoutYear) |&gt;\n  summarise(Checkouts = sum(Checkouts)) |&gt;\n  arrange(CheckoutYear) |&gt; \n  collect()\n\n# A tibble: 18 √ó 2\n   CheckoutYear Checkouts\n          &lt;int&gt;     &lt;int&gt;\n 1         2005   2129128\n 2         2006   3385869\n 3         2007   3679981\n 4         2008   4156859\n 5         2009   4500788\n 6         2010   4389760\n 7         2011   4484366\n 8         2012   4696376\n 9         2013   5394411\n10         2014   5606168\n11         2015   5784864\n12         2016   5915722\n13         2017   6280679\n14         2018   6831226\n15         2019   7339010\n16         2020   5549585\n17         2021   6659627\n18         2022   6301822"
  },
  {
    "objectID": "slides.html#gb-csv-file-arrow-dplyr-1",
    "href": "slides.html#gb-csv-file-arrow-dplyr-1",
    "title": "Intro to Arrow in R",
    "section": "9GB CSV file + arrow + dplyr",
    "text": "9GB CSV file + arrow + dplyr\n\nseattle_csv |&gt;\n  filter(endsWith(MaterialType, \"BOOK\")) |&gt;\n  group_by(CheckoutYear) |&gt;\n  summarise(Checkouts = sum(Checkouts)) |&gt;\n  arrange(CheckoutYear) |&gt; \n  collect() |&gt;\n  system.time()\n\n   user  system elapsed \n 43.194   3.268  38.781 \n\n\n42 million rows ‚Äì not bad, but could be faster‚Ä¶."
  },
  {
    "objectID": "slides.html#file-format-apache-parquet",
    "href": "slides.html#file-format-apache-parquet",
    "title": "Intro to Arrow in R",
    "section": "File Format: Apache Parquet",
    "text": "File Format: Apache Parquet\n\n\nhttps://parquet.apache.org/"
  },
  {
    "objectID": "slides.html#parquet",
    "href": "slides.html#parquet",
    "title": "Intro to Arrow in R",
    "section": "Parquet",
    "text": "Parquet\n\nusually smaller than equivalent CSV file\nrich type system & stores the data type along with the data\n‚Äúcolumn-oriented‚Äù == better performance over CSV‚Äôs row-by-row\n‚Äúrow-chunked‚Äù == work on different parts of the file at the same time or skip some chunks all together\n\n\n\nefficient encodings to keep file size down, and supports file compression, less data to move from disk to memory\nCSV has no info about data types, inferred by each parser"
  },
  {
    "objectID": "slides.html#parquet-files-row-chunked",
    "href": "slides.html#parquet-files-row-chunked",
    "title": "Intro to Arrow in R",
    "section": "Parquet Files: ‚Äúrow-chunked‚Äù",
    "text": "Parquet Files: ‚Äúrow-chunked‚Äù"
  },
  {
    "objectID": "slides.html#parquet-files-row-chunked-column-oriented",
    "href": "slides.html#parquet-files-row-chunked-column-oriented",
    "title": "Intro to Arrow in R",
    "section": "Parquet Files: ‚Äúrow-chunked & column-oriented‚Äù",
    "text": "Parquet Files: ‚Äúrow-chunked & column-oriented‚Äù"
  },
  {
    "objectID": "slides.html#writing-to-parquet",
    "href": "slides.html#writing-to-parquet",
    "title": "Intro to Arrow in R",
    "section": "Writing to Parquet",
    "text": "Writing to Parquet\n\nseattle_parquet &lt;- \"./data/seattle-library-checkouts-parquet\"\n\nseattle_csv |&gt;\n  write_dataset(path = seattle_parquet,\n                format = \"parquet\")"
  },
  {
    "objectID": "slides.html#storage-parquet-vs-csv",
    "href": "slides.html#storage-parquet-vs-csv",
    "title": "Intro to Arrow in R",
    "section": "Storage: Parquet vs CSV",
    "text": "Storage: Parquet vs CSV\n\nfile &lt;- list.files(seattle_parquet)\nfile.size(file.path(seattle_parquet, file)) / 10**9\n\n[1] 4.423348\n\n\n\nParquet about half the size of the CSV file on-disk üíæ"
  },
  {
    "objectID": "slides.html#file-storage-partitioning",
    "href": "slides.html#file-storage-partitioning",
    "title": "Intro to Arrow in R",
    "section": "File Storage:Partitioning",
    "text": "File Storage:Partitioning\n\n\n\nDividing data into smaller pieces, making it more easily accessible and manageable\n\n\n\n\n\nalso called multi-files or sometimes shards"
  },
  {
    "objectID": "slides.html#poll-partitioning",
    "href": "slides.html#poll-partitioning",
    "title": "Intro to Arrow in R",
    "section": "Poll: Partitioning?",
    "text": "Poll: Partitioning?\nHave you partitioned your data or used partitioned data before today?\n\n1Ô∏è‚É£ Yes\n2Ô∏è‚É£ No\n3Ô∏è‚É£ Not sure, the data engineers sort that out!"
  },
  {
    "objectID": "slides.html#art-science-of-partitioning",
    "href": "slides.html#art-science-of-partitioning",
    "title": "Intro to Arrow in R",
    "section": "Art & Science of Partitioning",
    "text": "Art & Science of Partitioning\n\n\navoid files &lt; 20MB and &gt; 2GB\navoid &gt; 10,000 files (ü§Ø)\npartition on variables used in filter()\n\n\n\nguidelines not rules, results vary\nexperiment\narrow suggests avoid files smaller than 20MB and larger than 2GB\navoid partitions that produce more than 10,000 files\npartition by variables that you filter by, allows arrow to only read relevant files"
  },
  {
    "objectID": "slides.html#rewriting-the-data-again",
    "href": "slides.html#rewriting-the-data-again",
    "title": "Intro to Arrow in R",
    "section": "Rewriting the Data Again",
    "text": "Rewriting the Data Again\n\nseattle_parquet_part &lt;- \"./data/seattle-library-checkouts\"\n\nseattle_csv |&gt;\n  group_by(CheckoutYear) |&gt;\n  write_dataset(path = seattle_parquet_part,\n                format = \"parquet\")"
  },
  {
    "objectID": "slides.html#what-did-we-engineer",
    "href": "slides.html#what-did-we-engineer",
    "title": "Intro to Arrow in R",
    "section": "What Did We ‚ÄúEngineer‚Äù?",
    "text": "What Did We ‚ÄúEngineer‚Äù?\n\nseattle_parquet_part &lt;- \"./data/seattle-library-checkouts\"\n\nsizes &lt;- tibble(\n  files = list.files(seattle_parquet_part, recursive = TRUE),\n  size_GB = file.size(file.path(seattle_parquet_part, files)) / 10**9\n)\n\nsizes\n\n# A tibble: 18 √ó 2\n   files                            size_GB\n   &lt;chr&gt;                              &lt;dbl&gt;\n 1 CheckoutYear=2005/part-0.parquet   0.114\n 2 CheckoutYear=2006/part-0.parquet   0.172\n 3 CheckoutYear=2007/part-0.parquet   0.186\n 4 CheckoutYear=2008/part-0.parquet   0.204\n 5 CheckoutYear=2009/part-0.parquet   0.224\n 6 CheckoutYear=2010/part-0.parquet   0.233\n 7 CheckoutYear=2011/part-0.parquet   0.250\n 8 CheckoutYear=2012/part-0.parquet   0.261\n 9 CheckoutYear=2013/part-0.parquet   0.282\n10 CheckoutYear=2014/part-0.parquet   0.296\n11 CheckoutYear=2015/part-0.parquet   0.308\n12 CheckoutYear=2016/part-0.parquet   0.315\n13 CheckoutYear=2017/part-0.parquet   0.319\n14 CheckoutYear=2018/part-0.parquet   0.306\n15 CheckoutYear=2019/part-0.parquet   0.302\n16 CheckoutYear=2020/part-0.parquet   0.158\n17 CheckoutYear=2021/part-0.parquet   0.240\n18 CheckoutYear=2022/part-0.parquet   0.252"
  },
  {
    "objectID": "slides.html#gb-partitioned-parquet-files-arrow-dplyr",
    "href": "slides.html#gb-partitioned-parquet-files-arrow-dplyr",
    "title": "Intro to Arrow in R",
    "section": "4.5GB partitioned Parquet files + arrow + dplyr",
    "text": "4.5GB partitioned Parquet files + arrow + dplyr\n\nseattle_parquet_part &lt;- \"./data/seattle-library-checkouts\"\n\nopen_dataset(seattle_parquet_part,\n             format = \"parquet\") |&gt;\n  filter(endsWith(MaterialType, \"BOOK\")) |&gt;\n  group_by(CheckoutYear) |&gt;\n  summarise(Checkouts = sum(Checkouts)) |&gt;\n  arrange(CheckoutYear) |&gt; \n  collect() |&gt;\n  system.time()\n\n   user  system elapsed \n 14.953   1.371   2.007 \n\n\n\n42 million rows ‚Äì not too shabby!"
  },
  {
    "objectID": "slides.html#partition-design",
    "href": "slides.html#partition-design",
    "title": "Intro to Arrow in R",
    "section": "Partition Design",
    "text": "Partition Design\n\n\n\nPartitioning on variables commonly used in filter() often faster\nNumber of partitions also important (Arrow reads the metadata of each file)"
  },
  {
    "objectID": "slides.html#performance-review-single-csv",
    "href": "slides.html#performance-review-single-csv",
    "title": "Intro to Arrow in R",
    "section": "Performance Review: Single CSV",
    "text": "Performance Review: Single CSV\nHow long does it take to calculate the number of books checked out in each month of 2021?\n\n\nopen_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n) |&gt; \n  filter(CheckoutYear == 2021, endsWith(MaterialType, \"BOOK\")) |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt;\n  system.time()\n\n   user  system elapsed \n 43.332   6.325  40.069"
  },
  {
    "objectID": "slides.html#performance-review-partitioned-parquet",
    "href": "slides.html#performance-review-partitioned-parquet",
    "title": "Intro to Arrow in R",
    "section": "Performance Review: Partitioned Parquet",
    "text": "Performance Review: Partitioned Parquet\nHow long does it take to calculate the number of books checked out in each month of 2021?\n\n\nopen_dataset(\"./data/seattle-library-checkouts\",\n             format = \"parquet\") |&gt; \n  filter(CheckoutYear == 2021, endsWith(MaterialType, \"BOOK\")) |&gt;\n  group_by(CheckoutMonth) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutMonth)) |&gt;\n  collect() |&gt; \n  system.time()\n\n   user  system elapsed \n  0.974   0.134   0.309"
  },
  {
    "objectID": "slides.html#engineering-data-tips-for-improved-storage-performance",
    "href": "slides.html#engineering-data-tips-for-improved-storage-performance",
    "title": "Intro to Arrow in R",
    "section": "Engineering Data Tips for Improved Storage & Performance",
    "text": "Engineering Data Tips for Improved Storage & Performance\n\n\nconsider ‚Äúcolumn-oriented‚Äù file formats like Parquet\nconsider partitioning, experiment to get an appropriate partition design üóÇÔ∏è\nwatch your schemas üëÄ"
  },
  {
    "objectID": "slides.html#getting-help-and-more-resources",
    "href": "slides.html#getting-help-and-more-resources",
    "title": "Intro to Arrow in R",
    "section": "Getting help and more resources",
    "text": "Getting help and more resources"
  },
  {
    "objectID": "slides.html#r-for-data-science-2e",
    "href": "slides.html#r-for-data-science-2e",
    "title": "Intro to Arrow in R",
    "section": "R for Data Science (2e)",
    "text": "R for Data Science (2e)\n\n\n\n\n\nChapter 23: Arrow\n\nhttps://r4ds.hadley.nz/"
  },
  {
    "objectID": "slides.html#scaling-up-with-r-and-arrow",
    "href": "slides.html#scaling-up-with-r-and-arrow",
    "title": "Intro to Arrow in R",
    "section": "Scaling Up with R and Arrow",
    "text": "Scaling Up with R and Arrow\n\n\n\n\nCurrently being written but preview available!\n\nhttps://arrowrbook.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Arrow in R",
    "section": "",
    "text": "Workshop Overview\nThis workshop will focus on using the arrow R package to process larger-than-memory files and multi-file datasets with arrow using familiar dplyr syntax. You‚Äôll learn to create and use interoperable data file formats like Parquet for efficient data storage and access, and also how to exercise fine control over data types to avoid common large data pipeline problems. This workshop will provide a foundation for using Arrow, giving you access to a powerful suite of tools for performant analysis of larger-than-memory data in R.\nThis course is for you if you:\n\nwant to learn how to work with tabular data that is too large to fit in memory using existing R and tidyverse syntax implemented in Arrow\nwant to learn about Parquet and other file formats that are powerful alternatives to CSV files\nwant to learn how to engineer your tabular data storage for more performant access and analysis with Apache Arrow\n\n\n\nWorkshop Prework\nDetailed instructions for software requirements and data sources are show below.\n\nPackages\nTo install the required core packages for the workshop, run the following:\n\ninstall.packages(c(\n  \"arrow\", \"dplyr\", \"stringr\", \"lubridate\", \"tictoc\"\n))\n\n\n\nSeattle Checkouts by Title Data\nThis is the data we will use in the workshop. It‚Äôs a good-sized, single CSV file‚Äî9GB on-disk in total, which can be downloaded from an AWS S3 bucket via https:\n\noptions(timeout = 1800)\ndownload.file(\n  url = \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  destfile = \"./data/seattle-library-checkouts.csv\"\n)\n\n\n\nTiny Data Option\nIf you don‚Äôt have time or disk space to download the 9Gb dataset (and still have disk space to do the exercises), you can run the code in the workshop with the ‚Äútiny‚Äù version of this data. Although the focus in this course is working with larger-than-memory data, you can still learn about the concepts and workflows with smaller data‚Äîalthough note you may not see the same performance improvements that you would get when working with larger data.\n\noptions(timeout = 1800)\ndownload.file(\n  url = \"https://github.com/posit-conf-2023/arrow/releases/download/v0.1.0/seattle-library-checkouts-tiny.csv\",\n  destfile = \"./data/seattle-library-checkouts-tiny.csv\"\n)\n\nIf you want to participate in the coding exercise or follow along, please try your very best to begin the workshop ready with the required software & packages installed and the data downloaded on to your laptop.\n\n This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "license-web.html",
    "href": "license-web.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  }
]