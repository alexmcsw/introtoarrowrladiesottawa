{
  "hash": "1eda6afc9dc3b61c94936a80f190c5b0",
  "result": {
    "markdown": "---\ntitle: Big Data in R with Arrow\nsubtitle: 2 hour Workshop\neditor: source\n---\n\n  \nContents written by Steph Hazlitt & Nic Crane\n\n### Workshop Overview\n  \nData analysis pipelines with larger-than-memory data are becoming more and more commonplace. In this workshop you will learn how to use Apache Arrow, a multi-language toolbox for working with larger-than-memory tabular data, to create seamless \"big\" data analysis pipelines with R.\n\nThe workshop will focus on using the the arrow R package---a mature R interface to Apache Arrow---to process larger-than-memory files and multi-file datasets with arrow using familiar dplyr syntax. You'll learn to create and use interoperable data file formats like Parquet for efficient data storage and access, and also how to exercise fine control over data types to avoid common large data pipeline problems. This workshop will provide a foundation for using Arrow, giving you access to a powerful suite of tools for performant analysis of larger-than-memory data in R.\n\n*This course is for you if you:*\n\n-   want to learn how to work with tabular data that is too large to fit in memory using existing R and tidyverse syntax implemented in Arrow\n-   want to learn about Parquet and other file formats that are powerful alternatives to CSV files\n-   want to learn how to engineer your tabular data storage for more performant access and analysis with Apache Arrow\n\n### Workshop Prework\n\nDetailed instructions for software requirements and data sources are show below.\n\n#### Packages\n\nTo install the required core packages for the workshop, run the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(c(\n  \"arrow\", \"dplyr\", \"stringr\", \"lubridate\", \"tictoc\"\n))\n```\n:::\n\n\n\n#### Seattle Checkouts by Title Data\n\nThis is the data we will use to explore data storage and engineering options. It's a good sized, single CSV file---*9GB* on-disk in total, which can be downloaded from the an AWS S3 bucket via https:\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(timeout = 1800)\ndownload.file(\n  url = \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  destfile = here::here(\"data/seattle-library-checkouts.csv\")\n)\n```\n:::\n\n\n#### Tiny Data Option\n\nIf you don't have time or disk space to download the 9Gb dataset (and still have disk space do the exercises), you can run the code and exercises in the course with \"tiny\" version of this data. Although the focus in this course is working with larger-than-memory data, you can still learn about the concepts and workflows with smaller data---although note you may not see the same performance improvements that you would get when working with larger data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(timeout = 1800)\ndownload.file(\n  url = \"https://github.com/posit-conf-2023/arrow/releases/download/v0.1.0/seattle-library-checkouts-tiny.csv\",\n  destfile = here::here(\"data/seattle-library-checkouts-tiny.csv\")\n)\n```\n:::\n\n\nIf you want to participate in the coding exercise or follow along, please try your very best to begin the workshop ready with the required software & packages installed and the data downloaded on to your laptop.\n\n### Instructors\n\n**Steph Hazlitt** is a data scientist, researcher and R enthusiast. She has spent the better part of her career wrangling data with R and supporting people and teams in learning, creating and sharing data science-related products and open source software.\n\n**Nic Crane** is a software engineer with a background in data science, and has a lot of enthusiasm for open source and learning and teaching all things R. Nic is part of the core team who maintain the Arrow R package.\n\n### Acknowledgements\n\nSome of the `Big Data in R with Arrow` workshop materials draw on other open-licensed teaching content which we would like to acknowledge:\n\n-   [useR!2022 virtual Larger-Than-Memory Data Workflows with Apache Arrow tutorial](https://github.com/djnavarro/arrow-user2022) authored by Danielle Navarro\n-   [R for Data Science (2e)](https://r4ds.hadley.nz/) written by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund---with thanks to Danielle Navarro who contributed the initial version of the [Arrow chapter](https://r4ds.hadley.nz/arrow)\n-   [How to use Arrow to work with large CSV files? blog post](https://francoismichonneau.net/2022/10/import-big-csv/) by François Michonneau, which introduces the single vs multi-file API models for learning/teaching Arrow\n\n------------------------------------------------------------------------\n\n![](https://i.creativecommons.org/l/by/4.0/88x31.png) This work is licensed under a [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}