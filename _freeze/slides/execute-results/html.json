{
  "hash": "00f2d0d00e9ec8d1074a6896bdb32d2d",
  "result": {
    "markdown": "---\nlogo: \"images/logo.png\"\nexecute:\n  echo: true\nformat:\n  revealjs: \n    theme: default\n    slide-number: true\n    footer: \"[🔗 https://github.com/thisisnic/introtoarrowworkshop](https://github.com/thisisnic/introtoarrowworkshop)\"\nengine: knitr\neditor: source\n---\n\n\n## Introduction to Arrow in R\n\n![](images/logo.png){fig-align=\"center\" width=\"500\" height=\"500\"}\n\n## Introductions\n\n-   Stephanie Hazlitt\n-   Nic Crane\n\nArrow contributors!\n\n::: notes\nHave participants introduce themselves in the chat\n:::\n\n## Welcome!\n\nToday we're going to cover:\n\n-   Working with larger-than-memory datasets with Arrow\n-   How to get the best performance with your tabular data\n-   Where to find more information\n\n## Workshop format\n\n-   Slides available at <https://tinyurl.com/introtoarrow>\n-   Follow-along coding\n-   Time to ask questions\n\n::: notes\n-   feel free to ask questions as we go along!\n:::\n\n## Getting set up\n\n![](images/newproj.png)\n\nRepository URL: <https://github.com/thisisnic/introtoarrowworkshop>\n\n## Dataset to follow along with\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(timeout = 1800)\ndownload.file(\n  url = \"https://r4ds.s3.us-west-2.amazonaws.com/seattle-library-checkouts.csv\",\n  destfile = \"./data/seattle-library-checkouts.csv\"\n)\n```\n:::\n\n\n::: notes\n-   dataset from R For Data Science\n-   run this from within the project you've got set up\n-   first line of code increase the download timeout to 30 mins - essential when downloading larger datasets\n-   if you don't have the data yet, download it now - next section talking about arrow\n:::\n\n## Backup option - Posit Cloud & Tiny Dataset\n\n# Part 1 - Arrow\n\n::: notes\n-   any questions before we get started?\n-   this section will cover a bit of background info about Apache Arrow\n:::\n\n## What is Apache Arrow?\n\n::: columns\n::: {.column width=\"50%\"}\n> A multi-language toolbox for accelerated data interchange and in-memory processing\n:::\n\n::: {.column width=\"50%\"}\n> Arrow is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another\n:::\n:::\n\n::: {style=\"font-size: 70%;\"}\n<https://arrow.apache.org/overview/>\n:::\n\n::: notes\n-   TODO: add main points want to hit here\n:::\n\n## Apache Arrow Specification\n\nIn-memory [columnar format]{style=\"background-color: #F3D1FF;\"}: a [standardized, language-agnostic specification]{style=\"background-color: #FFDCB7;\"} for representing structured, [table-like datasets]{style=\"background-color: #DCFFC9;\"} in-memory.\n\n<br>\n\n![](images/arrow-rectangle.png){.absolute left=\"200\"}\n\n::: notes\n-   brief overview of these points but coming back to later\n:::\n\n## A Multi-Language Toolbox\n\n![](images/arrow-libraries-structure.png)\n\n## Accelerated Data Interchange\n\n![](images/data-interchange-with-arrow.png)\n\n::: notes\n-   standardisation means multiple things speaking Arrow prevents copying back and forth\n:::\n\n## Accelerated In-Memory Processing\n\nArrow's Columnar Format is Fast\n\n![](images/columnar-fast.png){.absolute top=\"120\" left=\"200\" height=\"600\"}\n\n::: notes\n-   intro the toy dataset\n-   memory buffers are a 1-dimension structure, not like a 2D table/data.frame\n-   walk through this in the 2 diagrams\n-   analytic workflows typically have filtering, grouping by columns etc; give examples\n-   faster to scan adjacent areas than picking through it all + taking advantage of vectorization available on modern processes speeds up faster\n:::\n\n## arrow 📦\n\n<br>\n\n![](images/arrow-r-pkg-highlights.png){.absolute top=\"0\" left=\"300\" width=\"700\" height=\"900\"}\n\n## arrow 📦\n\n![](images/arrow-read-write-updated.png)\n\n::: notes\n-   different types of objects\n-   different file formats\n-   different storage locations\n:::\n\n# Part 2 - Working with Arrow Datasets\n\n::: notes\n-   any questions so far?\n-   maybe a poll - who's worked with dplyr/arrow/parquet before?\n:::\n\n## Seattle Checkouts - Big CSV\n\n![](images/seattle-checkouts.png){.absolute top=\"120\" left=\"200\" height=\"550\"}\n\n::: {style=\"font-size: 70%;\"}\n<https://data.seattle.gov/Community/Checkouts-by-Title/tmmm-ytt6/about_data>\n:::\n\n::: notes\n-   LIVE CODING\n    -   path to the data depending on where you've downloaded it\n    -   impact of data size on performance\n    -   walk through subcomponents of the output when print the `seattle_csv` object\n        -   explicitly mention the term \"schema\"\n        -   string and character are direct equivalents\n        -   why we have e.g. 64-bit integers\n        -   arrow automatically handles the conversion between R and Arrow data types\n    -   these types have been guessed from first 1MB of rows (can't say how many as varies with ncol)\n    -   can anyone spot something odd here?\n    -   what is an ISBN; what is the null type?\n    -   pulling out the schema\n    -   updating the schema\n    -   using the e.g. `string()` functions to create different data types and where in the docs?\n    -   show *both* schema updating and `col_types`\n    -   checking out the new schema\n    -   how often is this necessary? Often not. Good practice. Only important with CSVs not Parquet.\n    -   run `glimpse()` but remember it'll take a moment\n        -   TODO: find something to talk about here or don't use glimpse!\n        -   42 millions rows\n        -   UsageClass column data\n        -   ISBN column data\n        -   PublicationYear + why it's a string\n-   Recap this section!\n-   Questions?\n:::\n\n## How big is the dataset?\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\nlibrary(dplyr)\nfile.size(\"./data/seattle-library-checkouts.csv\") / 10 **9\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.211969\n```\n:::\n:::\n\n\n## Opening in Arrow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n)\n```\n:::\n\n\n## How many rows of data?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(seattle_csv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 41389465\n```\n:::\n:::\n\n\n## Extract schema\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschema(seattle_csv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n\n## Arrow Data Types\n\nArrow has a rich data type system, including direct analogs of many R data types\n\n-   `<dbl>` == `<double>`\n-   `<chr>` == `<string>` or `<utf8>`\n-   `<int>` == `<int32>`\n\n<br>\n\n<https://arrow.apache.org/docs/r/articles/data_types.html>\n\n## Parsing the Metadata\n\n<br>\n\nArrow scans 👀 1MB of the file(s) to impute or \"guess\" the data types\n\n::: {style=\"font-size: 80%; margin-top: 200px;\"}\n📚 arrow vs readr blog post: <https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/>\n:::\n\n## Parsers Are Not Always Right\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschema(seattle_csv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n\n![](images/data-dict.png){.absolute top=\"200\" left=\"330\" width=\"700\"}\n\n::: notes\nInternational Standard Book Number (ISBN) is a 13-digit number that uniquely identifies books and book-like products published internationally.\n\nData Dictionaries, metadata in data catalogues should provide this info.\n:::\n\n<!-- ## Let's Control the Schema -->\n\n<!-- <br> -->\n\n<!-- Creating a schema manually: -->\n\n<!-- ```{r} -->\n<!-- #| label: seattle-schema-write -->\n<!-- #| eval: false -->\n<!-- schema( -->\n<!--   UsageClass = utf8(), -->\n<!--   CheckoutType = utf8(), -->\n<!--   MaterialType = utf8(), -->\n<!--   ... -->\n<!-- ) -->\n<!-- ``` -->\n\n<!-- <br> -->\n\n<!-- This will take a lot of typing with 12 columns 😢 -->\n\n## Let's Control the Schema\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\n\nschema(seattle_csv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: string\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n\n<!-- ## Previewing the data -->\n\n<!-- ```{r} -->\n<!-- #| label: glimpse -->\n<!-- seattle_csv |> glimpse() -->\n<!-- ``` -->\n\n# Part 3 - Data Manipulation with Arrow\n\n::: notes\n-   \\<Up to here was about 28 mins in first practice run (no time for questions)\\>\n-   Question - how many people here have used dbplyr to connect to a database in R?\n:::\n\n## Arrow dplyr backend\n\n![](images/dplyr-backend.png)\n\n::: notes\nLIVE CODING - Data contains book, ebooks, things which aren't book - Do *not* call collect() after first query - talk about lazy eval and show the query - endsWith to ends_with; this is actually an arrow C++ lib func - Don't want to pull all into memory as it's a lot of data; preview using head - Now look at the outputs of that - Next: How many books and ebooks were checked out each year? - Don't need to call head() to preview as the data returned is just a row for each year - Run again with a timer set, then walk through results - Walk through the data, drop in 2020 - pandemic? - Not bad, it can be faster and this is what we'll talk about in part 3 RECAP - Any questions?\n:::\n\n## Querying the data - new column: is this a book?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  mutate(IsBook = endsWith(MaterialType, \"BOOK\")) |>\n  select(MaterialType, IsBook)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset (query)\nMaterialType: string\nIsBook: bool (ends_with(MaterialType, {pattern=\"BOOK\", ignore_case=false}))\n\nSee $.data for the source Arrow object\n```\n:::\n:::\n\n\n## Preview the query\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2,5\"}\nseattle_csv |>\n  head(20) |>\n  mutate(IsBook = endsWith(MaterialType, \"BOOK\")) |>\n  select(MaterialType, IsBook) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 2\n   MaterialType IsBook\n   <chr>        <lgl> \n 1 BOOK         TRUE  \n 2 BOOK         TRUE  \n 3 EBOOK        TRUE  \n 4 BOOK         TRUE  \n 5 SOUNDDISC    FALSE \n 6 BOOK         TRUE  \n 7 BOOK         TRUE  \n 8 EBOOK        TRUE  \n 9 BOOK         TRUE  \n10 EBOOK        TRUE  \n11 BOOK         TRUE  \n12 BOOK         TRUE  \n13 BOOK         TRUE  \n14 AUDIOBOOK    TRUE  \n15 BOOK         TRUE  \n16 EBOOK        TRUE  \n17 SOUNDDISC    FALSE \n18 VIDEODISC    FALSE \n19 SOUNDDISC    FALSE \n20 AUDIOBOOK    TRUE  \n```\n:::\n:::\n\n\n## How many books were checked out each year?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  filter(endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutYear) |>\n  summarise(Checkouts = sum(Checkouts)) |>\n  arrange(CheckoutYear) |> \n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 × 2\n   CheckoutYear Checkouts\n          <int>     <int>\n 1         2005   2129128\n 2         2006   3385869\n 3         2007   3679981\n 4         2008   4156859\n 5         2009   4500788\n 6         2010   4389760\n 7         2011   4484366\n 8         2012   4696376\n 9         2013   5394411\n10         2014   5606168\n11         2015   5784864\n12         2016   5915722\n13         2017   6280679\n14         2018   6831226\n15         2019   7339010\n16         2020   5549585\n17         2021   6659627\n18         2022   6301822\n```\n:::\n:::\n\n\n## How long did it take?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nseattle_csv |>\n  filter(endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutYear) |>\n  summarise(Checkouts = sum(Checkouts)) |>\n  arrange(CheckoutYear) |> \n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 11.909   1.031  10.767 \n```\n:::\n:::\n\n\n42 million rows -- not bad, but could be faster....\n\n# Part 4 - Engineering the Data\n\n## File Format: Apache Parquet\n\n![](images/apache-parquet.png){.absolute top=\"100\" left=\"200\" width=\"700\"}\n\n::: {style=\"font-size: 60%; margin-top: 450px;\"}\n<https://parquet.apache.org/>\n:::\n\n## Parquet\n\n-   compression and encoding - usually much smaller than equivalent CSV file\n-   rich type system & stores the schema along with the data\n-   \"column-oriented\" == better performance over CSV's row-by-row\n-   \"row-chunked\" == work on different parts of the file at the same time or skip some chunks all together\n\n::: notes\n-   CSVs widely used common format\n-   Parquet is a popular format for bigger data\n-   optimised for analytics workflows\n-   efficient encodings to keep file size down, and supports file compression, less data to move from disk to memory\n-   CSV has no info about data types, inferred by each parser - Parquet stores the type\n:::\n\n## Parquet Files: \"row-chunked\"\n\n![](images/parquet-chunking.png)\n\n## Parquet Files: \"row-chunked & column-oriented\"\n\n![](images/parquet-columnar.png)\n\n::: notes\nLIVE CODING -\n\nRECAP - Any questions?\n:::\n\n## Writing to Parquet\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_dir <- \"./data/seattle-library-checkouts-parquet\"\n\nseattle_csv |>\n  write_dataset(path = seattle_parquet_dir, format = \"parquet\")\n```\n:::\n\n\n## Storage: Parquet vs CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile <- list.files(seattle_parquet_dir, recursive = TRUE, full.names = TRUE)\nfile.size(file) / 10**9\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.424267\n```\n:::\n:::\n\n\n<br>\n\nParquet about half the size of the CSV file on-disk 💾\n\n## 4.5GB Parquet file + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(seattle_parquet_dir, \n             format = \"parquet\") |>\n  filter(endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutYear) |>\n  summarise(Checkouts = sum(Checkouts)) |>\n  arrange(CheckoutYear) |> \n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  4.436   0.558   0.944 \n```\n:::\n:::\n\n\n<br>\n\n42 million rows -- much better! But could be *even* faster....\n\n## File Storage: Partitioning\n\nDividing data into smaller pieces, making it more easily accessible and manageable\n\n![](images/partitioning.png){fig-align=\"right\"}\n\n::: notes\nalso called multi-files or sometimes shards\n:::\n\n## Poll: Partitioning?\n\nHave you partitioned your data or used partitioned data before today?\n\n-   1️⃣ Yes\n-   2️⃣ No\n-   3️⃣ Not sure, the data engineers sort that out!\n\n## Rewriting the Data Again\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- \"./data/seattle-library-checkouts\"\n\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = seattle_parquet_part,\n                format = \"parquet\")\n```\n:::\n\n\n## What Did We \"Engineer\"?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- \"./data/seattle-library-checkouts\"\n\nsizes <- tibble(\n  files = list.files(seattle_parquet_part, recursive = TRUE),\n  size_GB = round(file.size(file.path(seattle_parquet_part, files)) / 10**9, 3)\n)\n\nsizes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 × 2\n   files                            size_GB\n   <chr>                              <dbl>\n 1 CheckoutYear=2005/part-0.parquet   0.115\n 2 CheckoutYear=2006/part-0.parquet   0.172\n 3 CheckoutYear=2007/part-0.parquet   0.186\n 4 CheckoutYear=2008/part-0.parquet   0.204\n 5 CheckoutYear=2009/part-0.parquet   0.224\n 6 CheckoutYear=2010/part-0.parquet   0.233\n 7 CheckoutYear=2011/part-0.parquet   0.25 \n 8 CheckoutYear=2012/part-0.parquet   0.261\n 9 CheckoutYear=2013/part-0.parquet   0.282\n10 CheckoutYear=2014/part-0.parquet   0.296\n11 CheckoutYear=2015/part-0.parquet   0.308\n12 CheckoutYear=2016/part-0.parquet   0.315\n13 CheckoutYear=2017/part-0.parquet   0.319\n14 CheckoutYear=2018/part-0.parquet   0.306\n15 CheckoutYear=2019/part-0.parquet   0.303\n16 CheckoutYear=2020/part-0.parquet   0.158\n17 CheckoutYear=2021/part-0.parquet   0.24 \n18 CheckoutYear=2022/part-0.parquet   0.252\n```\n:::\n:::\n\n\n## 4.5GB partitioned Parquet files + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- \"./data/seattle-library-checkouts\"\n\nsystem.time(\n  books_by_year <- open_dataset(seattle_parquet_part,\n               format = \"parquet\") |>\n    filter(endsWith(MaterialType, \"BOOK\")) |>\n    group_by(CheckoutYear) |>\n    summarise(Checkouts = sum(Checkouts)) |>\n    arrange(CheckoutYear) |> \n    collect()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  4.140   0.486   0.652 \n```\n:::\n:::\n\n\n<br>\n\n42 million rows -- not too shabby!\n\n## Art & Science of Partitioning\n\n<br>\n\n-   avoid files \\< 20MB and \\> 2GB\n-   avoid \\> 10,000 files (🤯)\n-   partition on variables used in `filter()`\n\n::: notes\n-   guidelines not rules, results vary\n-   experiment\n-   arrow suggests avoid files smaller than 20MB and larger than 2GB\n-   avoid partitions that produce more than 10,000 files (Arrow reads the metadata of each file)\n-   partition by variables that you filter by, allows arrow to only read relevant files\n:::\n\n## Performance Review: Single CSV\n\nHow long does it take to calculate the number of books checked out in each month of 2021?\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n) |> \n  filter(CheckoutYear == 2021, endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 12.015   1.175  11.690 \n```\n:::\n:::\n\n\n## Performance Review: Partitioned Parquet\n\nHow long does it take to calculate the number of books checked out in each month of 2021?\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(\"./data/seattle-library-checkouts\",\n             format = \"parquet\") |> \n  filter(CheckoutYear == 2021, endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.255   0.041   0.070 \n```\n:::\n:::\n\n\n## Engineering Data Tips for Improved Storage & Performance\n\n<br>\n\n-   use Parquet over CSV if possible\n-   consider partitioning, experiment to get an appropriate partition design 🗂️\n-   watch your schemas 👀\n\n# Part 5 - More Resources\n\n## Arrow docs\n\n![](images/docs.png)\n\n## R for Data Science (2e)\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/r4ds-cover.jpg){.absolute top=\"100\" width=\"400\"}\n:::\n\n::: {.column width=\"50%\"}\n<br>\n\n[Chapter 23: Arrow](https://r4ds.hadley.nz/arrow.html)\n\n<br>\n\n<https://r4ds.hadley.nz/>\n:::\n:::\n\n## Scaling Up with R and Arrow\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/dummybookcover.png)\n:::\n\n::: {.column width=\"50%\"}\nCurrently being written - preview available online soon!\n\n<br>\n\n<https://arrowrbook.com>\n:::\n:::\n",
    "supporting": [
      "slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}