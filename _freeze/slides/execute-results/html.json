{
  "hash": "7df44f9096cb4adef5783a7c5514d7f8",
  "result": {
    "markdown": "---\nlogo: \"images/logo.png\"\nexecute:\n  echo: true\nformat:\n  revealjs: \n    theme: default\n    slide-number: true\nengine: knitr\n---\n\n\n## What is Apache Arrow?\n\n::: columns\n::: {.column width=\"50%\"}\n> A multi-language toolbox for accelerated data interchange and in-memory processing\n:::\n\n::: {.column width=\"50%\"}\n> Arrow is designed to both improve the performance of analytical algorithms and the efficiency of moving data from one system or programming language to another\n:::\n:::\n\n::: {style=\"font-size: 70%;\"}\n<https://arrow.apache.org/overview/>\n:::\n\n## Apache Arrow Specification\n\nIn-memory columnar format: a standardized, language-agnostic specification for representing structured, table-like data sets in-memory.\n\n<br>\n\n![](images/arrow-rectangle.png){.absolute left=\"200\"}\n\n## A Multi-Language Toolbox\n\n![](images/arrow-libraries-structure.png)\n\n## Accelerated Data Interchange\n\n![](images/data-interchange-with-arrow.png)\n\n## Accelerated In-Memory Processing\n\nArrow's Columnar Format is Fast\n\n![](images/columnar-fast.png){.absolute top=\"120\" left=\"200\" height=\"600\"}\n\n::: notes\nThe contiguous columnar layout enables vectorization using the latest SIMD (Single Instruction, Multiple Data) operations included in modern processors.\n:::\n\n## arrow 📦\n\n<br>\n\n![](images/arrow-r-pkg.png){.absolute top=\"0\" left=\"300\" width=\"700\" height=\"900\"}\n\n## arrow 📦\n\n![](images/arrow-read-write-updated.png)\n\n## Seattle<br>Checkouts<br>Big CSV\n\n![](images/seattle-checkouts.png){.absolute top=\"0\" left=\"300\"}\n\n## How big is the dataset?\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfs::file_size(\"./data/seattle-library-checkouts.csv\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n8.58G\n```\n:::\n:::\n\n\n## Opening in Arrow\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n)\n```\n:::\n\n\n## Extract schema\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschema(seattle_csv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n\n## Arrow Data Types\n\nArrow has a rich data type system, including direct analogs of many R data types\n\n-   `<dbl>` == `<double>`\n-   `<chr>` == `<string>` or `<utf8>`\n-   `<int>` == `<int32>`\n\n<br>\n\n<https://arrow.apache.org/docs/r/articles/data_types.html>\n\n## Parsing the Metadata\n\n<br>\n\nArrow scans 👀 a few thousand rows of the file(s) to impute or \"guess\" the data types\n\n::: {style=\"font-size: 80%; margin-top: 200px;\"}\n📚 arrow vs readr blog post: <https://thisisnic.github.io/2022/11/21/type-inference-in-readr-and-arrow/>\n:::\n\n## Parsers Are Not Always Right\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschema(seattle_csv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSchema\nUsageClass: string\nCheckoutType: string\nMaterialType: string\nCheckoutYear: int64\nCheckoutMonth: int64\nCheckouts: int64\nTitle: string\nISBN: null\nCreator: string\nSubjects: string\nPublisher: string\nPublicationYear: string\n```\n:::\n:::\n\n\n![](images/data-dict.png){.absolute top=\"300\" left=\"330\" width=\"700\"}\n\n::: notes\nInternational Standard Book Number (ISBN) is a 13-digit number that uniquely identifies books and book-like products published internationally.\n\nData Dictionaries, metadata in data catalogues should provide this info.\n:::\n\n## Let's Control the Schema\n\n<br>\n\nCreating a schema manually:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nschema(\n  UsageClass = utf8(),\n  CheckoutType = utf8(),\n  MaterialType = utf8(),\n  ...\n)\n```\n:::\n\n\n<br>\n\nThis will take a lot of typing with 12 columns 😢\n\n## Let's Control the Schema\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv <- open_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  col_types = schema(ISBN = string()),\n  format = \"csv\"\n)\n```\n:::\n\n\n## Previewing the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset with 1 csv file\n41,389,465 rows x 12 columns\n$ UsageClass      <string> \"Physical\", \"Physical\", \"Digital\", \"Physical\", \"Physi…\n$ CheckoutType    <string> \"Horizon\", \"Horizon\", \"OverDrive\", \"Horizon\", \"Horizo…\n$ MaterialType    <string> \"BOOK\", \"BOOK\", \"EBOOK\", \"BOOK\", \"SOUNDDISC\", \"BOOK\",…\n$ CheckoutYear     <int64> 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016,…\n$ CheckoutMonth    <int64> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ Checkouts        <int64> 1, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 2, 3, 2, 1, 3, 2, 3,…\n$ Title           <string> \"Super rich : a guide to having it all / Russell Simm…\n$ ISBN            <string> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"…\n$ Creator         <string> \"Simmons, Russell\", \"Barclay, James, 1965-\", \"Tim Par…\n$ Subjects        <string> \"Self realization, Conduct of life, Attitude Psycholo…\n$ Publisher       <string> \"Gotham Books,\", \"Pyr,\", \"Random House, Inc.\", \"Dial …\n$ PublicationYear <string> \"c2011.\", \"2010.\", \"2015\", \"2005.\", \"c2004.\", \"c2005.…\n```\n:::\n:::\n\n\n## Arrow dplyr backend\n\n![](images/dplyr-backend.png)\n\n## Querying the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  mutate(IsBook = endsWith(MaterialType, \"BOOK\")) |>\n  select(MaterialType, IsBook)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFileSystemDataset (query)\nMaterialType: string\nIsBook: bool (ends_with(MaterialType, {pattern=\"BOOK\", ignore_case=false}))\n\nSee $.data for the source Arrow object\n```\n:::\n:::\n\n\n## Preview the query\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2,5\"}\nseattle_csv |>\n  head(20) |>\n  mutate(IsBook = endsWith(MaterialType, \"BOOK\")) |>\n  select(MaterialType, IsBook) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 × 2\n   MaterialType IsBook\n   <chr>        <lgl> \n 1 BOOK         TRUE  \n 2 BOOK         TRUE  \n 3 EBOOK        TRUE  \n 4 BOOK         TRUE  \n 5 SOUNDDISC    FALSE \n 6 BOOK         TRUE  \n 7 BOOK         TRUE  \n 8 EBOOK        TRUE  \n 9 BOOK         TRUE  \n10 EBOOK        TRUE  \n11 BOOK         TRUE  \n12 BOOK         TRUE  \n13 BOOK         TRUE  \n14 AUDIOBOOK    TRUE  \n15 BOOK         TRUE  \n16 EBOOK        TRUE  \n17 SOUNDDISC    FALSE \n18 VIDEODISC    FALSE \n19 SOUNDDISC    FALSE \n20 AUDIOBOOK    TRUE  \n```\n:::\n:::\n\n\n## Data manipulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  filter(endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutYear) |>\n  summarise(Checkouts = sum(Checkouts)) |>\n  arrange(CheckoutYear) |> \n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 × 2\n   CheckoutYear Checkouts\n          <int>     <int>\n 1         2005   2129128\n 2         2006   3385869\n 3         2007   3679981\n 4         2008   4156859\n 5         2009   4500788\n 6         2010   4389760\n 7         2011   4484366\n 8         2012   4696376\n 9         2013   5394411\n10         2014   5606168\n11         2015   5784864\n12         2016   5915722\n13         2017   6280679\n14         2018   6831226\n15         2019   7339010\n16         2020   5549585\n17         2021   6659627\n18         2022   6301822\n```\n:::\n:::\n\n\n## 9GB CSV file + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_csv |>\n  filter(endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutYear) |>\n  summarise(Checkouts = sum(Checkouts)) |>\n  arrange(CheckoutYear) |> \n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 × 2\n   CheckoutYear Checkouts\n          <int>     <int>\n 1         2005   2129128\n 2         2006   3385869\n 3         2007   3679981\n 4         2008   4156859\n 5         2009   4500788\n 6         2010   4389760\n 7         2011   4484366\n 8         2012   4696376\n 9         2013   5394411\n10         2014   5606168\n11         2015   5784864\n12         2016   5915722\n13         2017   6280679\n14         2018   6831226\n15         2019   7339010\n16         2020   5549585\n17         2021   6659627\n18         2022   6301822\n```\n:::\n:::\n\n\n## 9GB CSV file + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"6\"}\nseattle_csv |>\n  filter(endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutYear) |>\n  summarise(Checkouts = sum(Checkouts)) |>\n  arrange(CheckoutYear) |> \n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 41.632   3.073  37.261 \n```\n:::\n:::\n\n\n42 million rows -- not bad, but could be faster....\n\n## File Format: Apache Parquet\n\n![](images/apache-parquet.png){.absolute top=\"100\" left=\"200\" width=\"700\"}\n\n::: {style=\"font-size: 60%; margin-top: 450px;\"}\n<https://parquet.apache.org/>\n:::\n\n## Parquet\n\n-   usually smaller than equivalent CSV file\n-   rich type system & stores the data type along with the data\n-   \"column-oriented\" == better performance over CSV's row-by-row\n-   \"row-chunked\" == work on different parts of the file at the same time or skip some chunks all together\n\n::: notes\n-   efficient encodings to keep file size down, and supports file compression, less data to move from disk to memory\n-   CSV has no info about data types, inferred by each parser\n:::\n\n## Parquet Files: \"row-chunked\"\n\n![](images/parquet-chunking.png)\n\n## Parquet Files: \"row-chunked & column-oriented\"\n\n![](images/parquet-columnar.png)\n\n## Writing to Parquet\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet <- \"./data/seattle-library-checkouts-parquet\"\n\nseattle_csv |>\n  write_dataset(path = seattle_parquet,\n                format = \"parquet\")\n```\n:::\n\n\n## Storage: Parquet vs CSV\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfile <- list.files(seattle_parquet)\nfile.size(file.path(seattle_parquet, file)) / 10**9\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4.423348\n```\n:::\n:::\n\n\n<br>\n\nParquet about half the size of the CSV file on-disk 💾\n\n## File Storage:<br>Partitioning\n\n<br>\n\n::: columns\n::: {.column width=\"50%\"}\nDividing data into smaller pieces, making it more easily accessible and manageable\n:::\n\n::: {.column width=\"50%\"}\n![](images/partitions.png){.absolute top=\"0\"}\n:::\n:::\n\n::: notes\nalso called multi-files or sometimes shards\n:::\n\n## Poll: Partitioning?\n\nHave you partitioned your data or used partitioned data before today?\n\n-   1️⃣ Yes\n-   2️⃣ No\n-   3️⃣ Not sure, the data engineers sort that out!\n\n## Art & Science of Partitioning\n\n<br>\n\n-   avoid files \\< 20MB and \\> 2GB\n-   avoid \\> 10,000 files (🤯)\n-   partition on variables used in `filter()`\n\n::: notes\n-   guidelines not rules, results vary\n-   experiment\n-   arrow suggests avoid files smaller than 20MB and larger than 2GB\n-   avoid partitions that produce more than 10,000 files\n-   partition by variables that you filter by, allows arrow to only read relevant files\n:::\n\n## Rewriting the Data Again\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- \"./data/seattle-library-checkouts\"\n\nseattle_csv |>\n  group_by(CheckoutYear) |>\n  write_dataset(path = seattle_parquet_part,\n                format = \"parquet\")\n```\n:::\n\n\n## What Did We \"Engineer\"?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- \"./data/seattle-library-checkouts\"\n\nsizes <- tibble(\n  files = list.files(seattle_parquet_part, recursive = TRUE),\n  size_GB = file.size(file.path(seattle_parquet_part, files)) / 10**9\n)\n\nsizes\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 18 × 2\n   files                            size_GB\n   <chr>                              <dbl>\n 1 CheckoutYear=2005/part-0.parquet   0.114\n 2 CheckoutYear=2006/part-0.parquet   0.172\n 3 CheckoutYear=2007/part-0.parquet   0.186\n 4 CheckoutYear=2008/part-0.parquet   0.204\n 5 CheckoutYear=2009/part-0.parquet   0.224\n 6 CheckoutYear=2010/part-0.parquet   0.233\n 7 CheckoutYear=2011/part-0.parquet   0.250\n 8 CheckoutYear=2012/part-0.parquet   0.261\n 9 CheckoutYear=2013/part-0.parquet   0.282\n10 CheckoutYear=2014/part-0.parquet   0.296\n11 CheckoutYear=2015/part-0.parquet   0.308\n12 CheckoutYear=2016/part-0.parquet   0.315\n13 CheckoutYear=2017/part-0.parquet   0.319\n14 CheckoutYear=2018/part-0.parquet   0.306\n15 CheckoutYear=2019/part-0.parquet   0.302\n16 CheckoutYear=2020/part-0.parquet   0.158\n17 CheckoutYear=2021/part-0.parquet   0.240\n18 CheckoutYear=2022/part-0.parquet   0.252\n```\n:::\n:::\n\n\n## 4.5GB partitioned Parquet files + arrow + dplyr\n\n\n::: {.cell}\n\n```{.r .cell-code}\nseattle_parquet_part <- \"./data/seattle-library-checkouts\"\n\nopen_dataset(seattle_parquet_part,\n             format = \"parquet\") |>\n  filter(endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutYear) |>\n  summarise(Checkouts = sum(Checkouts)) |>\n  arrange(CheckoutYear) |> \n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 15.907   1.738   2.593 \n```\n:::\n:::\n\n\n<br>\n\n42 million rows -- not too shabby!\n\n## Partition Design\n\n::: columns\n::: {.column width=\"50%\"}\n-   Partitioning on variables commonly used in `filter()` often faster\n-   Number of partitions also important (Arrow reads the metadata of each file)\n:::\n\n::: {.column width=\"50%\"}\n![](images/partitions.png){.absolute top=\"0\"}\n:::\n:::\n\n## Performance Review: Single CSV\n\nHow long does it take to calculate the number of books checked out in each month of 2021?\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(\n  sources = \"./data/seattle-library-checkouts.csv\", \n  format = \"csv\"\n) |> \n  filter(CheckoutYear == 2021, endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n 43.384   6.315  40.200 \n```\n:::\n:::\n\n\n## Performance Review: Partitioned Parquet\n\nHow long does it take to calculate the number of books checked out in each month of 2021?\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code}\nopen_dataset(\"./data/seattle-library-checkouts\",\n             format = \"parquet\") |> \n  filter(CheckoutYear == 2021, endsWith(MaterialType, \"BOOK\")) |>\n  group_by(CheckoutMonth) |>\n  summarize(TotalCheckouts = sum(Checkouts)) |>\n  arrange(desc(CheckoutMonth)) |>\n  collect() |> \n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.991   0.130   0.278 \n```\n:::\n:::\n\n\n## Engineering Data Tips for Improved Storage & Performance\n\n<br>\n\n-   consider \"column-oriented\" file formats like Parquet\n-   consider partitioning, experiment to get an appropriate partition design 🗂️\n-   watch your schemas 👀\n\n## Getting help and more resources\n\n<!-- Add in some links here -->\n\n## R for Data Science (2e)\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/r4ds-cover.jpg){.absolute top=\"100\" width=\"400\"}\n:::\n\n::: {.column width=\"50%\"}\n<br>\n\n[Chapter 23: Arrow](https://r4ds.hadley.nz/arrow.html)\n\n<br>\n\n<https://r4ds.hadley.nz/>\n:::\n:::\n\n## Scaling Up with R and Arrow\n\n::: columns\n::: {.column width=\"50%\"}\n![](images/dummybookcover.png)\n:::\n\n::: {.column width=\"50%\"}\n[Currently being written but preview available!](https://arrowrbook.com)\n\n<br>\n\n<https://arrowrbook.com>\n:::\n:::\n",
    "supporting": [
      "slides_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}